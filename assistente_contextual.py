import os
import json
import requests

# --- SE√á√ÉO 1: FUN√á√ÉO DE COMUNICA√á√ÉO COM O GATEWAY (Sem altera√ß√µes) ---

def chamar_servidor_gateway(endpoint: str, payload: dict) -> str:
    """Fun√ß√£o centralizada para chamar endpoints do nosso servidor gateway."""
    try:
        # Usando o nome do servi√ßo Docker para a comunica√ß√£o
        url = f"http://ia_gateway:8000/{endpoint.strip('/')}"
        response = requests.post(url, json=payload, timeout=300)
        response.raise_for_status()
        return response.json().get("texto_gerado", f"ERRO: Resposta inv√°lida do endpoint /{endpoint}")
    except requests.exceptions.RequestException as e:
        return f"ERRO DE CONEX√ÉO com o Servidor Gateway: {e}"

# --- SE√á√ÉO 2: FUN√á√ïES PARA CRIA√á√ÉO DE PROMPT (Sem altera√ß√µes) ---

def criar_prompt_tecnico(instrucoes_json: dict, pergunta_usuario: str) -> str:
    """
    Constr√≥i um prompt detalhado combinando as instru√ß√µes do prompts.json
    com a pergunta do usu√°rio.
    """
    persona = instrucoes_json.get("instructions", {}).get("persona", {})
    regras = instrucoes_json.get("instructions", {}).get("rules", [])
    objetivo = instrucoes_json.get("objective", "")
    regras_formatadas = "\n".join(regras)
    prompt_final = f"""
Voc√™ √© um agente especialista. Siga estritamente as instru√ß√µes, regras e persona definidas abaixo para formular sua resposta.

### Persona: {persona.get('title', 'Assistente Especialista')}
{persona.get('description', '')}

### Objetivo Principal
{objetivo}

### Regras a Seguir
{regras_formatadas}

### Pergunta do Usu√°rio:
"{pergunta_usuario}"

### Resposta Especialista:
"""
    return prompt_final.strip()

# --- SE√á√ÉO 3: L√ìGICA DE CHAT (Atualizada para receber o nome do modelo) ---

def loop_chat_com_prompt_tecnico(instrucoes_base: dict, nome_modelo: str):
    """
    Loop de chat que utiliza o prompt t√©cnico antes de cada chamada.
    """
    agent_name = instrucoes_base.get("agent_name", "Agente")
    print(f"\n‚úÖ Chat iniciado com o agente especialista: '{agent_name}'.")
    print(f"   -> Modelo em uso: {nome_modelo}") # Exibe o nome do modelo
    print("   Digite 'sair' a qualquer momento para terminar.")
    
    while True:
        pergunta_usuario = input("\nü§ñ Voc√™ pergunta: ")
        if pergunta_usuario.strip().lower() == 'sair':
            break
        
        print("   -> Construindo prompt t√©cnico...")
        prompt_completo = criar_prompt_tecnico(instrucoes_base, pergunta_usuario)
        
        print("   ...pensando (via servidor gateway)...")
        response = chamar_servidor_gateway("gerar", {"prompt": prompt_completo})
        
        print("\nüí° Resposta do Especialista:")
        print(response)

# --- EXECU√á√ÉO PRINCIPAL (Atualizada para carregar o nome do modelo) ---

if __name__ == "__main__":
    print("--- Assistente de IA com Prompt T√©cnico ---")
    
    PROMPTS_CONFIG = {}
    nome_modelo_ativo = "N√£o especificado"

    try:
        # Carrega o arquivo de prompts que define o comportamento do agente
        with open("prompts.json", 'r', encoding='utf-8') as f:
            PROMPTS_CONFIG = json.load(f)
        print("‚úÖ Instru√ß√µes do agente carregadas de 'prompts.json'.")

        # --- IN√çCIO DA CORRE√á√ÉO ---
        # Carrega o arquivo de configura√ß√£o para descobrir o nome do modelo
        with open("config_modelo_local.json", 'r', encoding='utf-8') as f:
            CONFIG = json.load(f)
        
        # Assume que o gerador principal est√° configurado para a nuvem
        serv_princ_cfg = CONFIG.get("servicos", {}).get("gerador_principal", {})
        nome_modelo_ativo = serv_princ_cfg.get('id_openrouter', "Modelo de Nuvem Padr√£o")
        print(f"‚úÖ Nome do modelo ('{nome_modelo_ativo}') carregado de 'config_modelo_local.json'.")
        # --- FIM DA CORRE√á√ÉO ---

    except FileNotFoundError as e:
        print(f"‚ùå ERRO CR√çTICO: Arquivo de configura√ß√£o '{e.filename}' n√£o encontrado.")
        exit()
    except json.JSONDecodeError as e:
        print(f"‚ùå ERRO CR√çTICO: Arquivo de configura√ß√£o cont√©m um erro de formata√ß√£o (JSON inv√°lido).")
        exit()

    # Inicia o loop de chat, passando as instru√ß√µes e o nome do modelo
    loop_chat_com_prompt_tecnico(PROMPTS_CONFIG, nome_modelo_ativo)
        
    print("\nüëã Sess√£o encerrada. At√© logo!")